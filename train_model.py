import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, BertConfig
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
from sklearn.model_selection import train_test_split
from torch.optim import AdamW
from tqdm import tqdm

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA Version: {torch.version.cuda}")

# ê²Œì„ ì¶”ì²œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class GameQADataset(Dataset):
    def __init__(self, questions, answers, tokenizer, max_length=512):
        self.questions = questions
        self.answers = answers
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        question = self.questions[idx]
        answer = self.answers[idx]

        # BERT QA í˜•ì‹ì— ë§ê²Œ ì…ë ¥ êµ¬ì„±
        inputs = self.tokenizer(
            question,
            answer,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # start_positionsê³¼ end_positions ì°¾ê¸°
        answer_tokens = self.tokenizer.encode(answer, add_special_tokens=False)
        answer_start = inputs['input_ids'][0].tolist().index(answer_tokens[0])
        answer_end = answer_start + len(answer_tokens) - 1

        return {
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'start_positions': torch.tensor(answer_start),
            'end_positions': torch.tensor(answer_end)
        }

def train_model(csv_file_path="augmented_game_qa_data.csv", model_save_path="./model"):
    print("ê²Œì„ ì¶”ì²œ BERT ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 60)
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    model_name = "klue/bert-base"
    max_seq_length = 300
    batch_size = 2
    learning_rate = 5e-5
    epochs = 3
    seed = 42
    
    # ëœë¤ ì‹œë“œ ê³ ì •
    torch.manual_seed(seed)
    np.random.seed(seed)
    
    # ë°ì´í„° ë¡œë“œ
    try:
        df = pd.read_csv(csv_file_path)
        print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í•­ëª©")
        
        if len(df) == 0:
            print("âŒ CSV íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return False
            
        if 'question' not in df.columns or 'answer' not in df.columns:
            print("âŒ CSV íŒŒì¼ì— 'question', 'answer' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print(f"í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")
            return False
            
        print(f"ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
        print(f"- ì§ˆë¬¸ ì˜ˆì‹œ: {df['question'].iloc[0]}")
        print(f"- ë‹µë³€ ì˜ˆì‹œ: {df['answer'].iloc[0][:50]}...")
        print()
            
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return False
    
    # í† í¬ë‚˜ì´ì € ì¤€ë¹„
    print("ğŸ”¤ í† í¬ë‚˜ì´ì €ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=300)
        print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    
    # ë°ì´í„°ë¥¼ train/valë¡œ ë¶„í• 
    train_df, val_df = train_test_split(df, test_size=0.1, random_state=seed)
    
    print(f"ë°ì´í„° ë¶„í• :")
    print(f"- í›ˆë ¨ ë°ì´í„°: {len(train_df)}ê°œ")
    print(f"- ê²€ì¦ ë°ì´í„°: {len(val_df)}ê°œ")
    print()
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„
    train_dataset = GameQADataset(
        train_df['question'].tolist(),
        train_df['answer'].tolist(),
        tokenizer,
        max_seq_length
    )
    val_dataset = GameQADataset(
        val_df['question'].tolist(),
        val_df['answer'].tolist(),
        tokenizer,
        max_seq_length
    )
    
    # ë°ì´í„°ë¡œë” ì¤€ë¹„
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4 if torch.cuda.is_available() else 0
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4 if torch.cuda.is_available() else 0
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print("BERT ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    try:
        model = AutoModelForQuestionAnswering.from_pretrained(model_name)
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    
    # ëª¨ë¸ì„ GPUë¡œ ì´ë™
    model = model.to(device)
    
    # í›ˆë ¨ ì‹œì‘
    print("\ní›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 60)
    
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        model.train()
        total_train_loss = 0
        train_steps = 0
        
        # í›ˆë ¨ ë£¨í”„
        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs} [Train]')
        for batch in train_pbar:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            start_positions = batch['start_positions'].to(device)
            end_positions = batch['end_positions'].to(device)
            
            optimizer.zero_grad()
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                start_positions=start_positions,
                end_positions=end_positions
            )
            
            loss = outputs.loss
            
            # ì†ì‹¤ì´ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ì—­ì „íŒŒ
            if not torch.isnan(loss) and not torch.isinf(loss):
                total_train_loss += loss.item()
                train_steps += 1
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
            
            train_pbar.set_postfix({'loss': loss.item() if not torch.isnan(loss) else 'nan'})
        
        avg_train_loss = total_train_loss / train_steps if train_steps > 0 else float('inf')
        
        # ê²€ì¦ ë£¨í”„
        model.eval()
        total_val_loss = 0
        val_steps = 0
        
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch + 1}/{epochs} [Val]')
            for batch in val_pbar:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                start_positions = batch['start_positions'].to(device)
                end_positions = batch['end_positions'].to(device)
                
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    start_positions=start_positions,
                    end_positions=end_positions
                )
                
                loss = outputs.loss
                if not torch.isnan(loss) and not torch.isinf(loss):
                    total_val_loss += loss.item()
                    val_steps += 1
                
                val_pbar.set_postfix({'loss': loss.item() if not torch.isnan(loss) else 'nan'})
        
        avg_val_loss = total_val_loss / val_steps if val_steps > 0 else float('inf')
        
        print(f"\nEpoch {epoch + 1}/{epochs}")
        print(f"Average training loss: {avg_train_loss:.4f}")
        print(f"Average validation loss: {avg_val_loss:.4f}")
        
        # ëª¨ë¸ ì €ì¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            if not os.path.exists(model_save_path):
                os.makedirs(model_save_path)
            model.save_pretrained(model_save_path)
            tokenizer.save_pretrained(model_save_path)
            print(f"âœ… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {model_save_path}")
    
    print("\ní›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    return True

if __name__ == "__main__":
    train_model() 