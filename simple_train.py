import torch
import pandas as pd
from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig
import os
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from tqdm import tqdm

class SimpleGameDataset(Dataset):
    def __init__(self, questions, answers, tokenizer, max_length=128):
        self.questions = questions
        self.answers = answers
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.questions)
    
    def __getitem__(self, idx):
        question = str(self.questions[idx])
        answer = str(self.answers[idx])
        
        # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•¨ê»˜ ì¸ì½”ë”©
        encoding = self.tokenizer.encode_plus(
            question,
            answer,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # ê°„ë‹¨í•œ start/end ìœ„ì¹˜ ì„¤ì •
        input_ids = encoding['input_ids'].flatten()
        attention_mask = encoding['attention_mask'].flatten()
        
        # [SEP] í† í° ìœ„ì¹˜ ì°¾ê¸°
        sep_token_id = self.tokenizer.sep_token_id
        sep_positions = (input_ids == sep_token_id).nonzero().flatten()
        
        if len(sep_positions) >= 2:
            start_pos = sep_positions[0].item() + 1
            end_pos = min(sep_positions[1].item() - 1, self.max_length - 1)
        else:
            start_pos = len(self.tokenizer.encode(question, add_special_tokens=True))
            end_pos = min(start_pos + 10, self.max_length - 1)
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'start_positions': torch.tensor(start_pos, dtype=torch.long),
            'end_positions': torch.tensor(end_pos, dtype=torch.long)
        }

def simple_train(csv_file="game_qa_data.csv", model_dir="./model", epochs=3, batch_size=4):
    print("ğŸ® ê°„ë‹¨í•œ ê²Œì„ ì¶”ì²œ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    
    # ë°ì´í„° ë¡œë“œ
    if not os.path.exists(csv_file):
        print(f"âŒ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_file}")
        return False
    
    df = pd.read_csv(csv_file)
    print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ")
    
    # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
    print("ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model_name = "beomi/kcbert-base"
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertForQuestionAnswering.from_pretrained(model_name)
    
    # ë°ì´í„° ë¶„í• 
    questions = df['question'].tolist()
    answers = df['answer'].tolist()
    
    train_questions, val_questions, train_answers, val_answers = train_test_split(
        questions, answers, test_size=0.2, random_state=42
    )
    
    # ë°ì´í„°ì…‹ê³¼ ë°ì´í„°ë¡œë”
    train_dataset = SimpleGameDataset(train_questions, train_answers, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # ì˜µí‹°ë§ˆì´ì €
    optimizer = AdamW(model.parameters(), lr=2e-5)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    print(f"ğŸš€ í›ˆë ¨ ì‹œì‘ (ë””ë°”ì´ìŠ¤: {device})")
    
    # í›ˆë ¨ ë£¨í”„
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch in progress_bar:
            optimizer.zero_grad()
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            start_positions = batch['start_positions'].to(device)
            end_positions = batch['end_positions'].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                start_positions=start_positions,
                end_positions=end_positions
            )
            
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            progress_bar.set_postfix({'loss': loss.item()})
        
        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1} ì™„ë£Œ - í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
    
    # ëª¨ë¸ ì €ì¥
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {model_dir}")
    os.makedirs(model_dir, exist_ok=True)
    
    model.save_pretrained(model_dir)
    tokenizer.save_pretrained(model_dir)
    
    print("âœ… í›ˆë ¨ ì™„ë£Œ!")
    return True

if __name__ == "__main__":
    success = simple_train()
    if success:
        print("\nğŸ‰ ì„±ê³µ! ì´ì œ ì›¹ì„œë¹„ìŠ¤ë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”:")
        print("python app.py")
    else:
        print("\nâŒ í›ˆë ¨ ì‹¤íŒ¨") 