# predict.py

import torch
import pandas as pd
from transformers import BertTokenizer, BertForQuestionAnswering
import os
from typing import List, Dict
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from ratsnlp import nlpbook
from ratsnlp.nlpbook.qa import QADeployArguments

class GameRecommendationPredictor:
    def __init__(self, model_path="./model", qa_data_path="augmented_game_qa_data.csv"):
        self.model_path = model_path
        
        # ratsnlp ì„¤ì •
        self.args = QADeployArguments(
            pretrained_model_name="beomi/kcbert-base",
            downstream_model_dir=model_path,
            max_seq_length=512,
            max_query_length=64
        )
        
        self.model_loaded = False
        
        # QA ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        try:
            self.qa_data = pd.read_csv(qa_data_path)
            print(f"âœ… QA ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.qa_data)}ê°œ í•­ëª©")
            
            # TF-IDF ë²¡í„°í™”ë¥¼ ìœ„í•œ ì¤€ë¹„
            self.vectorizer = TfidfVectorizer()
            self.qa_vectors = self.vectorizer.fit_transform(self.qa_data['question'].fillna(''))
            print("âœ… TF-IDF ë²¡í„°í™” ì™„ë£Œ")
            
        except FileNotFoundError:
            print(f"âš ï¸ QA ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {qa_data_path}")
            raise FileNotFoundError(f"í›ˆë ¨ìš© ë°ì´í„° íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤: {qa_data_path}")
        
        # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ (í•„ìˆ˜)
        self.load_trained_model()
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if self.model_loaded:
            self.model.to(self.device)
            self.model.eval()
            print(f"ğŸš€ ëª¨ë¸ì´ {self.device}ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    
    def load_trained_model(self):
        """í›ˆë ¨ëœ ëª¨ë¸ì„ ë¡œë“œ (í•„ìˆ˜ ìš”êµ¬ì‚¬í•­)"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(
                f"âŒ í›ˆë ¨ëœ ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {self.model_path}\n"
                f"ğŸ’¡ ë¨¼ì € ë‹¤ìŒì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•´ì£¼ì„¸ìš”:\n"
                f"   python train_model.py"
            )
        
        try:
            print("ğŸ¤– í›ˆë ¨ëœ BERT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            self.tokenizer = BertTokenizer.from_pretrained(self.model_path)
            self.model = BertForQuestionAnswering.from_pretrained(self.model_path)
            self.model_loaded = True
            print("âœ… í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            raise RuntimeError(
                f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n"
                f"ğŸ’¡ ëª¨ë¸ íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í›ˆë ¨í•´ì£¼ì„¸ìš”:\n"
                f"   python train_model.py"
            )
    
    def find_best_context(self, question: str) -> str:
        """ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ ë°˜í™˜"""
        # ì§ˆë¬¸ì„ TF-IDF ë²¡í„°ë¡œ ë³€í™˜
        query_vector = self.vectorizer.transform([question])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = cosine_similarity(query_vector, self.qa_vectors).flatten()
        
        # ê°€ì¥ ìœ ì‚¬í•œ ìƒìœ„ 5ê°œ ë‹µë³€ ì„ íƒ
        top_indices = similarities.argsort()[-5:][::-1]
        top_answers = self.qa_data.iloc[top_indices]['answer'].tolist()
        
        # ì„ íƒëœ ë‹µë³€ë“¤ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        return " ".join(top_answers)
    
    def format_answer(self, answer: str) -> str:
        """ë‹µë³€ì„ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…"""
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = [s.strip() for s in answer.split('.') if s.strip()]
        
        # í¬ë§·íŒ…ëœ ë‹µë³€ ìƒì„±
        formatted = "ğŸ® ê²Œì„ ì¶”ì²œ ê²°ê³¼:\n\n"
        
        # ê° ë¬¸ì¥ì„ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ í‘œì‹œí•˜ê³ , ê¸´ ë¬¸ì¥ì€ ì—¬ëŸ¬ ì¤„ë¡œ ë‚˜ëˆ„ê¸°
        for i, sentence in enumerate(sentences, 1):
            # ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ ì ì ˆíˆ ë‚˜ëˆ„ê¸°
            if len(sentence) > 100:
                words = sentence.split()
                current_line = []
                current_length = 0
                
                for word in words:
                    if current_length + len(word) + 1 > 100:
                        formatted += f"â€¢ {' '.join(current_line)}\n"
                        current_line = [word]
                        current_length = len(word)
                    else:
                        current_line.append(word)
                        current_length += len(word) + 1
                
                if current_line:
                    formatted += f"â€¢ {' '.join(current_line)}\n"
            else:
                formatted += f"â€¢ {sentence}.\n"
        
        # ì¶”ê°€ ì •ë³´
        formatted += "\nğŸ’¡ ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"
        
        return formatted

    def inference_fn(self, question: str, context: str = None) -> str:
        """í›ˆë ¨ëœ BERT ëª¨ë¸ì„ ì´ìš©í•œ ê²Œì„ ì¶”ì²œ ì¶”ë¡ """
        if not question or not question.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
        if not self.model_loaded:
            raise RuntimeError("âŒ í›ˆë ¨ëœ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        # ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ìë™ìœ¼ë¡œ ì°¾ê¸°
        if context is None:
            context = self.find_best_context(question)
        
        try:
            # ì§ˆë¬¸ í† í°í™” ë° ìë¥´ê¸°
            truncated_query = self.tokenizer.encode(
                question,
                add_special_tokens=False,
                truncation=True,
                max_length=self.args.max_query_length
            )
            
            # ì…ë ¥ ì¸ì½”ë”©
            inputs = self.tokenizer.encode_plus(
                text=truncated_query,
                text_pair=context,
                truncation="only_second",
                padding="max_length",
                max_length=self.args.max_seq_length,
                return_token_type_ids=True,
            )
            
            # í…ì„œë¡œ ë³€í™˜ ë° ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            model_inputs = {k: torch.tensor([v]).to(self.device) for k, v in inputs.items()}
            
            # BERT ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                outputs = self.model(**model_inputs)
                start_pred = outputs.start_logits.argmax(dim=-1).item()
                end_pred = outputs.end_logits.argmax(dim=-1).item()
                
                # ë‹µë³€ ì¶”ì¶œ
                if start_pred <= end_pred and end_pred < len(inputs['input_ids']) and start_pred >= 0:
                    pred_text = self.tokenizer.decode(
                        inputs['input_ids'][start_pred:end_pred+1],
                        skip_special_tokens=True
                    ).strip()
                    
                    if pred_text and len(pred_text) > 2:
                        return self.format_answer(pred_text)
                    else:
                        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
                else:
                    return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”."
                
        except Exception as e:
            raise RuntimeError(f"âŒ ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ì „ì—­ ì˜ˆì¸¡ê¸° ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_predictor = None

def get_answer(question: str) -> str:
    """Flask ì•±ì—ì„œ ì‚¬ìš©í•  ì¸í„°í˜ì´ìŠ¤ - í›ˆë ¨ëœ ëª¨ë¸ë§Œ ì‚¬ìš©"""
    global _predictor
    
    try:
        if _predictor is None:
            _predictor = GameRecommendationPredictor()
        
        return _predictor.inference_fn(question)
    
    except (FileNotFoundError, RuntimeError) as e:
        error_msg = str(e)
        print(error_msg)
        return f"âŒ ëª¨ë¸ ì˜¤ë¥˜: í›ˆë ¨ëœ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € 'python train_model.py'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
    
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        return "âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
